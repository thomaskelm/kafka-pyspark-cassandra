{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations related to Kafka connector \n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0 pyspark-shell'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import json_tuple,from_json,explode,col\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from pyspark.sql.types import StructType,FloatType, StructField, StringType,TimestampType\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"SparkStreaming\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a streaming DataFrame that reads from topic1\n",
    "socketDF = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "  .option(\"subscribe\", \"incoming_tweets\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"created_at\",TimestampType(), True),\\\n",
    "        StructField(\"text\",StringType(), True),\\\n",
    "        StructField(\"id\",StringType(), True),\\\n",
    "        StructField(\"sentimentScore\",FloatType(), True),\\\n",
    "        StructField(\"sentiment\",StringType(), True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json = socketDF.selectExpr(\"CAST(value AS STRING) as data\")\n",
    "\n",
    "json = json.withColumn(\"data\",from_json(json.data,schema))\\\n",
    "    \n",
    "data = json.select(col('data.*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=data.writeStream\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .format(\"memory\")\\\n",
    "    .queryName(\"data\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "avgSentiment = spark.sql(\"\"\"\n",
    "SELECT avg(sentimentScore), date_format(window(from_utc_timestamp(created_at,'cst'), '1 minutes').start, 'YYYY-MM-dd HH:mm:ss') as time \n",
    "                              FROM data\n",
    "                              GROUP BY window(from_utc_timestamp(created_at,'cst'), '1 minutes')\n",
    "                              ORDER BY time \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalCounts = spark.sql(\"\"\"\n",
    "SELECT count(*) as count,count(distinct id) as idCount FROM data\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalCounts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetSample = spark.sql(\"\"\"\n",
    "SELECT * FROM data\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetSample.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentCounts = spark.sql(\"\"\"\n",
    "SELECT count(*) as count,sentiment, date_format(window(from_utc_timestamp(created_at,'cst'), '1 minutes').start, 'YYYY-MM-dd HH:mm:ss') as time \n",
    "                              FROM data\n",
    "                              GROUP BY window(from_utc_timestamp(created_at,'cst'), '1 minutes'),sentiment\n",
    "                              ORDER BY time desc\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "starttime=time.time()\n",
    "while True:\n",
    "    sentimentCounts.show()\n",
    "    avgSentiment.show()\n",
    "    time.sleep(5.0- ((time.time() - starttime) % 5.0))\n",
    "    clear_output()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import datetime\n",
    "plt.style.use('classic')\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "while True:\n",
    "    print(datetime.datetime.fromtimestamp(time.time()).strftime('%H:%M:%S'))\n",
    "    fig, ax = plt.subplots(2,1,figsize=(18, 16))\n",
    "    xfmt = mdates.DateFormatter('%d-%m-%y %H:%M')\n",
    "\n",
    "\n",
    "    avgSentiment = spark.sql(\"\"\"\n",
    "    SELECT avg(sentimentScore), date_format(window(from_utc_timestamp(created_at,'cst'), '1 minutes').start, 'YYYY-MM-dd HH:mm:ss') as time \n",
    "                                  FROM data\n",
    "                                  GROUP BY window(from_utc_timestamp(created_at,'cst'), '1 minutes')\n",
    "                                  ORDER BY time \n",
    "    \"\"\")\n",
    "    \n",
    "    sentimentCounts = spark.sql(\"\"\"\n",
    "        SELECT count(*) as count,sentiment, date_format(window(from_utc_timestamp(created_at,'cst'), '1 minutes').start, 'YYYY-MM-dd HH:mm:ss') as time \n",
    "                                      FROM data\n",
    "                                      GROUP BY window(from_utc_timestamp(created_at,'cst'), '1 minutes'),sentiment\n",
    "                                      ORDER BY time desc\n",
    "        \"\"\")\n",
    "    ax[0].clear()\n",
    "    ax[1].clear()\n",
    "    df = avgSentiment.toPandas()\n",
    "    df['time'] = pd.to_datetime(df['time']) - pd.Timedelta('06:00:00')\n",
    "\n",
    "    df.set_index('time').plot(ax=ax[0])\n",
    "    df2 = sentimentCounts.toPandas()\n",
    "    df2['time'] = pd.to_datetime(df2['time']) - pd.Timedelta('06:00:00')\n",
    "    \n",
    "    for key,grp in df2.groupby('sentiment'):\n",
    "        grp.plot(ax=ax[1],x='time',y='count', label=key)\n",
    "    \n",
    "    ax[0].legend(loc='best')\n",
    "    ax[1].legend(loc='best')\n",
    "    ax[0].set_ylim([-10, 10])\n",
    "    ax[1].set_ylim([0,5+ax[1].get_ylim()[1]])\n",
    "    \n",
    "    ax[0].set_xlim([ax[0].get_xlim()[1]-0.01,0.01+ax[0].get_xlim()[1]])\n",
    "    ax[1].set_xlim( [ax[1].get_xlim()[1]-0.01,0.01+ax[1].get_xlim()[1]])\n",
    "    \n",
    "    fig.autofmt_xdate()\n",
    "    \n",
    "    ax[0].xaxis.set_major_formatter(xfmt)\n",
    "    ax[1].xaxis.set_major_formatter(xfmt)\n",
    "    \n",
    "    fig.canvas.draw()\n",
    "    plt.pause(5)\n",
    "    fig.canvas.flush_events()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgSentiment.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentCounts.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
